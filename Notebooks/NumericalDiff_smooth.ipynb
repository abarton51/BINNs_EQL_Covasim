{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Differentiation of Denoised Data from Sample Mean of Multiple Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import joblib\n",
    "import datetime\n",
    "sys.path.append('../')\n",
    "\n",
    "from Modules.Utils.ModelWrapper import ModelWrapper\n",
    "from Modules.Models.BuildBINNs import MLPComponentsCovasim, MLPComponentsCovasim2\n",
    "from Modules.Utils.Imports import *\n",
    "\n",
    "import Modules.Loaders.DataFormatter as DF\n",
    "from utils import plot_loss_convergence, get_case_name\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(GetLowestGPU(pick_from=[0,1,2,3]))\n",
    "\n",
    "def to_torch(ndarray):\n",
    "    arr = torch.tensor(ndarray, dtype=torch.float)\n",
    "    arr.requires_grad_(True)\n",
    "    arr = arr.to(device)\n",
    "    return arr\n",
    "\n",
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate BINN model parameters and path\n",
    "path = '../Data/covasim_data/drums_data/'\n",
    "population = 50000\n",
    "test_prob = 0.1\n",
    "trace_prob = 0.3\n",
    "keep_d = True\n",
    "retrain = False\n",
    "dynamic = True\n",
    "n_runs = 1000\n",
    "chi_type = 'piecewise'\n",
    "case_name = get_case_name(population, test_prob, trace_prob, keep_d, dynamic=dynamic, chi_type=chi_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = DF.load_covasim_data(path, population, test_prob, trace_prob, keep_d, case_name + '_' + str(n_runs), plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(params['data'])\n",
    "data_smooth = np.mean(data, axis=0)\n",
    "data_smooth = (data_smooth / params['population'])\n",
    "\n",
    "N = len(data_smooth)\n",
    "\n",
    "smooth_data = to_torch(data_smooth[1:N-1,:])\n",
    "\n",
    "t_max = N - 1\n",
    "t = np.arange(1, N - 1)[:,None]\n",
    "# indices = (t - 1)[:,0]\n",
    "params.pop('data')\n",
    "\n",
    "# computer numerically approximated derivatives\n",
    "M_front = data_smooth[:N-2,:]\n",
    "M_back = data_smooth[2:,:]\n",
    "derivs = (M_back - M_front) / 2.\n",
    "ut = to_torch(derivs)\n",
    "\n",
    "# split into train/val and convert to torch\n",
    "split = int(0.8*N)\n",
    "# generate shuffled array of indices from 0 to N-1\n",
    "p = np.random.permutation(N-2) + 1\n",
    "# assign x_train to be randomly shuffled days from 1 to 182 of size int(0.8 * N)\n",
    "x_train = to_torch((p[:split][:,None] + 1)/(N-1))\n",
    "# assign y_train to be values corresponding to x_train of size int(0.8 * N)\n",
    "y_train = to_torch(data_smooth[p[:split]])\n",
    "# assign x_val to be randomly shuffled days from 1 to 182 of size int(0.2 * N)\n",
    "x_val = to_torch((p[split:][:,None] + 1)/(N-1))\n",
    "# assign y_val to be values corresponding to y_val of size int(0.2 * N)\n",
    "y_val = to_torch(data_smooth[p[split:]])\n",
    "\n",
    "tracing_array = params['tracing_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 9)\n",
      "torch.Size([181, 9])\n",
      "(181, 1)\n",
      "183\n",
      "146\n",
      "(181,)\n",
      "1\n",
      "181\n"
     ]
    }
   ],
   "source": [
    "print(data_smooth.shape)\n",
    "print(smooth_data.shape)\n",
    "print(t.shape)\n",
    "print(N)\n",
    "print(split)\n",
    "print(p.shape)\n",
    "print(p.min())\n",
    "print(p.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_fit(t, u):\n",
    "    \n",
    "    return u[p - 1]\n",
    "\n",
    "def est_deriv(t, ut):\n",
    "    \n",
    "    return ut[p - 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- The shape of `derivs` is $(181, 9)$ since we used the method of central differences to estimate the derivatives for data of shape $(183, 9)$.\n",
    "- `t` are the time points of each of the `derivs` points.\n",
    "- `indices` is the array of the index points for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSdt = derivs[:,0]\n",
    "dTdt = derivs[:,1]\n",
    "dEdt = derivs[:,2]\n",
    "dAdt = derivs[:,3]\n",
    "dYdt = derivs[:,4]\n",
    "dDdt = derivs[:,5]\n",
    "dQdt = derivs[:,6]\n",
    "dRdt = derivs[:,7]\n",
    "dFdt = derivs[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teddy\\AppData\\Local\\Temp\\ipykernel_14948\\1775666219.py:3: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "plot = False\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "if plot:\n",
    "    for i in range(9):\n",
    "        if i==0:\n",
    "            plt.title('$dSdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dSdt', color='b')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dSdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dSdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==1:\n",
    "            plt.title('$dTdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dTdt*50000, label='dTdt', color='b')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dTdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dTdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==2:\n",
    "            plt.title('$dEdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dEdt', color='y')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dEdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dSEt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==3:\n",
    "            plt.title('$dAdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dAdt', color='r')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dAdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dAdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==4:\n",
    "            plt.title('$dYdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dYdt', color='r')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dYdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dYdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==5:\n",
    "            plt.title('$dDdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dDdt', color='m')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dDdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dDdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==6:\n",
    "            plt.title('$dQdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dQdt', color='m')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dQdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dQdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==7:\n",
    "            plt.title('$dRdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dRdt', color='g')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dRdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dRdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==8:\n",
    "            plt.title('$dFdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dFdt', color='k')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dFdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dFdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate save path\n",
    "mydir = os.path.join('../models/covasim', datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(mydir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPComponentsCovasim(\n",
       "  (surface_fitter): identity_MLP(\n",
       "    (mlp): BuildMLP(\n",
       "      (activation): Identity()\n",
       "      (output_activation): Identity()\n",
       "      (MLP): Sequential(\n",
       "        (0): Linear(in_features=1, out_features=9, bias=True)\n",
       "        (1): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (eta_func): infect_rate_MLP(\n",
       "    (mlp): BuildMLP(\n",
       "      (activation): ReLU()\n",
       "      (output_activation): Sigmoid()\n",
       "      (MLP): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (beta_func): beta_MLP(\n",
       "    (mlp): BuildMLP(\n",
       "      (activation): ReLU()\n",
       "      (output_activation): Sigmoid()\n",
       "      (MLP): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tau_func): tau_MLP(\n",
       "    (mlp): BuildMLP(\n",
       "      (activation): ReLU()\n",
       "      (output_activation): Sigmoid()\n",
       "      (MLP): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "binn = MLPComponentsCovasim(params, smooth_data, ut, N - 1, tracing_array, keep_d=keep_d, chi_type=chi_type)\n",
    "binn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = binn.parameters()\n",
    "opt = torch.optim.Adam(parameters, lr=1e-3)\n",
    "os.makedirs(os.path.join(mydir, case_name))\n",
    "model = ModelWrapper(\n",
    "    model=binn,\n",
    "    optimizer=opt,\n",
    "    loss=binn.loss,\n",
    "    augmentation=None,\n",
    "    # scheduler= scheduler,\n",
    "    save_name=os.path.join(mydir, case_name) )\n",
    "model.str_name = 'STEAYDQRF_no_main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the range information before training\n",
    "ranges = [binn.yita_lb, binn.yita_ub, binn.beta_lb, binn.beta_ub, binn.tau_lb, binn.tau_ub]\n",
    "file_name = '_'.join([str(m) for m in ranges])\n",
    "joblib.dump(None, os.path.join(mydir, file_name)) # model.save_folder\n",
    "# if retrain\n",
    "if retrain:\n",
    "    model.load(model.save_name + '_best_val_model', device=device)\n",
    "    model.model.train()\n",
    "    model.save_name += '_retrain'\n",
    "    \n",
    "epochs = int(2000)\n",
    "batch_size = 128\n",
    "rel_save_thresh = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([181, 9])\n",
      "torch.Size([145, 9])\n",
      "torch.Size([181, 9])\n",
      "torch.Size([1000, 9])\n",
      "1000\n",
      "t : tensor([[0.7778],\n",
      "        [0.9212],\n",
      "        [0.9025],\n",
      "        [0.0384],\n",
      "        [0.7892],\n",
      "        [0.5904],\n",
      "        [0.8662],\n",
      "        [0.1463],\n",
      "        [0.6868],\n",
      "        [0.9225],\n",
      "        [0.6622],\n",
      "        [0.4886],\n",
      "        [0.4591],\n",
      "        [0.9696],\n",
      "        [0.0232],\n",
      "        [0.7266],\n",
      "        [0.5754],\n",
      "        [0.6682],\n",
      "        [0.9534],\n",
      "        [0.6033],\n",
      "        [0.0266],\n",
      "        [0.5316],\n",
      "        [0.3454],\n",
      "        [0.3730],\n",
      "        [0.5972],\n",
      "        [0.1189],\n",
      "        [0.8115],\n",
      "        [0.6116],\n",
      "        [0.6395],\n",
      "        [0.2010],\n",
      "        [0.7556],\n",
      "        [0.9495],\n",
      "        [0.7864],\n",
      "        [0.1594],\n",
      "        [0.0466],\n",
      "        [0.4128],\n",
      "        [0.7350],\n",
      "        [0.6155],\n",
      "        [0.9228],\n",
      "        [0.9658],\n",
      "        [0.5522],\n",
      "        [0.3597],\n",
      "        [0.7982],\n",
      "        [0.1435],\n",
      "        [0.1023],\n",
      "        [0.7709],\n",
      "        [0.3191],\n",
      "        [0.9120],\n",
      "        [0.8163],\n",
      "        [0.8734],\n",
      "        [0.3803],\n",
      "        [0.7877],\n",
      "        [0.9978],\n",
      "        [0.5643],\n",
      "        [0.9440],\n",
      "        [0.2146],\n",
      "        [0.0563],\n",
      "        [0.9659],\n",
      "        [0.7013],\n",
      "        [0.9357],\n",
      "        [0.4291],\n",
      "        [0.7414],\n",
      "        [0.6757],\n",
      "        [0.5403],\n",
      "        [0.6750],\n",
      "        [0.4638],\n",
      "        [0.8634],\n",
      "        [0.7594],\n",
      "        [0.3009],\n",
      "        [0.5250],\n",
      "        [0.3492],\n",
      "        [0.2042],\n",
      "        [0.9373],\n",
      "        [0.0580],\n",
      "        [0.9946],\n",
      "        [0.8294],\n",
      "        [0.1103],\n",
      "        [0.2874],\n",
      "        [0.0954],\n",
      "        [0.0120],\n",
      "        [0.5285],\n",
      "        [0.2745],\n",
      "        [0.0776],\n",
      "        [0.4395],\n",
      "        [0.5460],\n",
      "        [0.1622],\n",
      "        [0.5246],\n",
      "        [0.1075],\n",
      "        [0.8790],\n",
      "        [0.6020],\n",
      "        [0.4551],\n",
      "        [0.4262],\n",
      "        [0.3435],\n",
      "        [0.6246],\n",
      "        [0.5460],\n",
      "        [0.7966],\n",
      "        [0.3595],\n",
      "        [0.2242],\n",
      "        [0.1354],\n",
      "        [0.2435],\n",
      "        [0.2664],\n",
      "        [0.1345],\n",
      "        [0.3082],\n",
      "        [0.6202],\n",
      "        [0.4953],\n",
      "        [0.2969],\n",
      "        [0.3671],\n",
      "        [0.9707],\n",
      "        [0.7827],\n",
      "        [0.4685],\n",
      "        [0.9460],\n",
      "        [0.2467],\n",
      "        [0.0423],\n",
      "        [0.5866],\n",
      "        [0.8466],\n",
      "        [0.7999],\n",
      "        [0.3755],\n",
      "        [0.6379],\n",
      "        [0.2564],\n",
      "        [0.0483],\n",
      "        [0.8158],\n",
      "        [0.3646],\n",
      "        [0.9429],\n",
      "        [0.9380],\n",
      "        [0.1957],\n",
      "        [0.8065],\n",
      "        [0.0992],\n",
      "        [0.4082],\n",
      "        [0.2401],\n",
      "        [0.5970],\n",
      "        [0.3352],\n",
      "        [0.8257],\n",
      "        [0.1500],\n",
      "        [0.2933],\n",
      "        [0.2630],\n",
      "        [0.0453],\n",
      "        [0.8839],\n",
      "        [0.7134],\n",
      "        [0.4879],\n",
      "        [0.3324],\n",
      "        [0.3334],\n",
      "        [0.6464],\n",
      "        [0.1123],\n",
      "        [0.9598],\n",
      "        [0.3265],\n",
      "        [0.8257],\n",
      "        [0.1289],\n",
      "        [0.7639],\n",
      "        [0.3002],\n",
      "        [0.5418],\n",
      "        [0.9873],\n",
      "        [0.0354],\n",
      "        [0.8670],\n",
      "        [0.7547],\n",
      "        [0.2077],\n",
      "        [0.8066],\n",
      "        [0.3305],\n",
      "        [0.0368],\n",
      "        [0.5176],\n",
      "        [0.6406],\n",
      "        [0.9169],\n",
      "        [0.1016],\n",
      "        [0.7386],\n",
      "        [0.4929],\n",
      "        [0.4327],\n",
      "        [0.2592],\n",
      "        [0.7815],\n",
      "        [0.2924],\n",
      "        [0.5402],\n",
      "        [0.8203],\n",
      "        [0.7576],\n",
      "        [0.1625],\n",
      "        [0.9259],\n",
      "        [0.5029],\n",
      "        [0.6863],\n",
      "        [0.3060],\n",
      "        [0.2646],\n",
      "        [0.7495],\n",
      "        [0.9005],\n",
      "        [0.8711],\n",
      "        [0.1985],\n",
      "        [0.4162],\n",
      "        [0.2775],\n",
      "        [0.8651],\n",
      "        [0.4594],\n",
      "        [0.7193],\n",
      "        [0.4273],\n",
      "        [0.4898],\n",
      "        [0.3928],\n",
      "        [0.3159],\n",
      "        [0.5867],\n",
      "        [0.6879],\n",
      "        [0.0216],\n",
      "        [0.9516],\n",
      "        [0.1042],\n",
      "        [0.1558],\n",
      "        [0.2708],\n",
      "        [0.2610],\n",
      "        [0.0140],\n",
      "        [0.7262],\n",
      "        [0.4002],\n",
      "        [0.6744],\n",
      "        [0.9206],\n",
      "        [0.9151],\n",
      "        [0.9995],\n",
      "        [0.5991],\n",
      "        [0.1486],\n",
      "        [0.6061],\n",
      "        [0.4991],\n",
      "        [0.0214],\n",
      "        [0.1485],\n",
      "        [0.5405],\n",
      "        [0.7828],\n",
      "        [0.9945],\n",
      "        [0.3145],\n",
      "        [0.7313],\n",
      "        [0.2216],\n",
      "        [0.4969],\n",
      "        [0.3808],\n",
      "        [0.7728],\n",
      "        [0.7687],\n",
      "        [0.8081],\n",
      "        [0.3701],\n",
      "        [0.5963],\n",
      "        [0.9160],\n",
      "        [0.7559],\n",
      "        [0.4274],\n",
      "        [0.4318],\n",
      "        [0.9214],\n",
      "        [0.7758],\n",
      "        [0.3444],\n",
      "        [0.4680],\n",
      "        [0.5108],\n",
      "        [0.5923],\n",
      "        [0.1805],\n",
      "        [0.8836],\n",
      "        [0.7374],\n",
      "        [0.6087],\n",
      "        [0.8971],\n",
      "        [0.2813],\n",
      "        [0.5423],\n",
      "        [0.2810],\n",
      "        [0.8503],\n",
      "        [0.0307],\n",
      "        [0.6842],\n",
      "        [0.7831],\n",
      "        [0.9988],\n",
      "        [0.8925],\n",
      "        [0.4685],\n",
      "        [0.8820],\n",
      "        [0.1011],\n",
      "        [0.3716],\n",
      "        [0.2294],\n",
      "        [0.8212],\n",
      "        [0.5909],\n",
      "        [0.7750],\n",
      "        [0.8173],\n",
      "        [0.7583],\n",
      "        [0.4628],\n",
      "        [0.1756],\n",
      "        [0.6895],\n",
      "        [0.1217],\n",
      "        [0.9646],\n",
      "        [0.3296],\n",
      "        [0.5245],\n",
      "        [0.7796],\n",
      "        [0.2090],\n",
      "        [0.8226],\n",
      "        [0.3994],\n",
      "        [0.6932],\n",
      "        [0.3128],\n",
      "        [0.4087],\n",
      "        [0.5929],\n",
      "        [0.7684],\n",
      "        [0.6927],\n",
      "        [0.5025],\n",
      "        [0.6424],\n",
      "        [0.0642],\n",
      "        [0.8754],\n",
      "        [0.6089],\n",
      "        [0.7945],\n",
      "        [0.8114],\n",
      "        [0.7780],\n",
      "        [0.9521],\n",
      "        [0.1427],\n",
      "        [0.8721],\n",
      "        [0.2731],\n",
      "        [0.8960],\n",
      "        [0.5088],\n",
      "        [0.5588],\n",
      "        [0.0728],\n",
      "        [0.0614],\n",
      "        [0.8505],\n",
      "        [0.8045],\n",
      "        [0.4833],\n",
      "        [0.0630],\n",
      "        [0.7678],\n",
      "        [0.1316],\n",
      "        [0.6267],\n",
      "        [0.8986],\n",
      "        [0.3811],\n",
      "        [0.8190],\n",
      "        [0.7999],\n",
      "        [0.5236],\n",
      "        [0.4353],\n",
      "        [0.3101],\n",
      "        [0.5146],\n",
      "        [0.4713],\n",
      "        [0.7100],\n",
      "        [0.7868],\n",
      "        [0.9664],\n",
      "        [0.2413],\n",
      "        [0.4943],\n",
      "        [0.6559],\n",
      "        [0.2382],\n",
      "        [0.7670],\n",
      "        [0.7998],\n",
      "        [0.6979],\n",
      "        [0.7012],\n",
      "        [0.2833],\n",
      "        [0.6310],\n",
      "        [0.2277],\n",
      "        [0.1983],\n",
      "        [0.6027],\n",
      "        [0.9318],\n",
      "        [0.0960],\n",
      "        [0.5575],\n",
      "        [0.8386],\n",
      "        [0.4550],\n",
      "        [0.3910],\n",
      "        [0.9122],\n",
      "        [0.1995],\n",
      "        [0.4341],\n",
      "        [0.9547],\n",
      "        [0.6331],\n",
      "        [0.7341],\n",
      "        [0.3963],\n",
      "        [0.3066],\n",
      "        [0.7049],\n",
      "        [0.5590],\n",
      "        [0.6822],\n",
      "        [0.5395],\n",
      "        [0.4104],\n",
      "        [0.8560],\n",
      "        [0.3968],\n",
      "        [0.7326],\n",
      "        [0.7589],\n",
      "        [0.2569],\n",
      "        [0.6610],\n",
      "        [0.4452],\n",
      "        [0.4393],\n",
      "        [0.8679],\n",
      "        [0.3529],\n",
      "        [0.6145],\n",
      "        [0.8003],\n",
      "        [0.4772],\n",
      "        [0.3441],\n",
      "        [0.8378],\n",
      "        [0.0934],\n",
      "        [0.6920],\n",
      "        [0.5924],\n",
      "        [0.5802],\n",
      "        [0.2177],\n",
      "        [0.1310],\n",
      "        [0.0215],\n",
      "        [0.7618],\n",
      "        [0.4233],\n",
      "        [0.0049],\n",
      "        [0.8002],\n",
      "        [0.8412],\n",
      "        [0.8875],\n",
      "        [0.5718],\n",
      "        [0.3153],\n",
      "        [0.1688],\n",
      "        [0.3049],\n",
      "        [0.2685],\n",
      "        [0.5381],\n",
      "        [0.6953],\n",
      "        [0.5555],\n",
      "        [0.5266],\n",
      "        [0.0839],\n",
      "        [0.0753],\n",
      "        [0.8225],\n",
      "        [0.0020],\n",
      "        [0.1302],\n",
      "        [0.4794],\n",
      "        [0.1401],\n",
      "        [0.3658],\n",
      "        [0.2659],\n",
      "        [0.4438],\n",
      "        [0.8818],\n",
      "        [0.5276],\n",
      "        [0.8032],\n",
      "        [0.9999],\n",
      "        [0.0388],\n",
      "        [0.3641],\n",
      "        [0.1795],\n",
      "        [0.5971],\n",
      "        [0.4956],\n",
      "        [0.0889],\n",
      "        [0.1573],\n",
      "        [0.1419],\n",
      "        [0.0141],\n",
      "        [0.8844],\n",
      "        [0.8328],\n",
      "        [0.3179],\n",
      "        [0.3447],\n",
      "        [0.1659],\n",
      "        [0.5209],\n",
      "        [0.9067],\n",
      "        [0.4514],\n",
      "        [0.7689],\n",
      "        [0.3852],\n",
      "        [0.2403],\n",
      "        [0.7378],\n",
      "        [0.1173],\n",
      "        [0.5421],\n",
      "        [0.4864],\n",
      "        [0.3289],\n",
      "        [0.0929],\n",
      "        [0.2021],\n",
      "        [0.5893],\n",
      "        [0.0369],\n",
      "        [0.1027],\n",
      "        [0.1318],\n",
      "        [0.5614],\n",
      "        [0.6880],\n",
      "        [0.4392],\n",
      "        [0.7046],\n",
      "        [0.8742],\n",
      "        [0.5203],\n",
      "        [0.8863],\n",
      "        [0.0997],\n",
      "        [0.0423],\n",
      "        [0.7045],\n",
      "        [0.6675],\n",
      "        [0.5786],\n",
      "        [0.3335],\n",
      "        [0.6986],\n",
      "        [0.2153],\n",
      "        [0.4542],\n",
      "        [0.3373],\n",
      "        [0.7527],\n",
      "        [0.3862],\n",
      "        [0.9171],\n",
      "        [0.2722],\n",
      "        [0.7569],\n",
      "        [0.0772],\n",
      "        [0.0649],\n",
      "        [0.1132],\n",
      "        [0.7389],\n",
      "        [0.3335],\n",
      "        [0.5751],\n",
      "        [0.7973],\n",
      "        [0.4198],\n",
      "        [0.8400],\n",
      "        [0.3929],\n",
      "        [0.3181],\n",
      "        [0.2936],\n",
      "        [0.3726],\n",
      "        [0.5168],\n",
      "        [0.3932],\n",
      "        [0.2280],\n",
      "        [0.4893],\n",
      "        [0.6093],\n",
      "        [0.7821],\n",
      "        [0.2591],\n",
      "        [0.9844],\n",
      "        [0.4352],\n",
      "        [0.0413],\n",
      "        [0.8920],\n",
      "        [0.2795],\n",
      "        [0.5689],\n",
      "        [0.7720],\n",
      "        [0.4732],\n",
      "        [0.0887],\n",
      "        [0.7984],\n",
      "        [0.0278],\n",
      "        [0.2349],\n",
      "        [0.5395],\n",
      "        [0.0660],\n",
      "        [0.0847],\n",
      "        [0.3273],\n",
      "        [0.3195],\n",
      "        [0.5442],\n",
      "        [0.0113],\n",
      "        [0.6918],\n",
      "        [0.6420],\n",
      "        [0.6610],\n",
      "        [0.0175],\n",
      "        [0.6828],\n",
      "        [0.7839],\n",
      "        [0.8265],\n",
      "        [0.4171],\n",
      "        [0.8185],\n",
      "        [0.2709],\n",
      "        [0.2619],\n",
      "        [0.5804],\n",
      "        [0.4294],\n",
      "        [0.0549],\n",
      "        [0.3371],\n",
      "        [0.4267],\n",
      "        [0.5695],\n",
      "        [0.2774],\n",
      "        [0.1804],\n",
      "        [0.8284],\n",
      "        [0.2406],\n",
      "        [0.6725],\n",
      "        [0.6724],\n",
      "        [0.5208],\n",
      "        [0.6993],\n",
      "        [0.3405],\n",
      "        [0.3408],\n",
      "        [0.7510],\n",
      "        [0.3735],\n",
      "        [0.6743],\n",
      "        [0.2902],\n",
      "        [0.8880],\n",
      "        [0.4677],\n",
      "        [0.7173],\n",
      "        [0.5185],\n",
      "        [0.4129],\n",
      "        [0.7659],\n",
      "        [0.7939],\n",
      "        [0.5234],\n",
      "        [0.0184],\n",
      "        [0.1739],\n",
      "        [0.4789],\n",
      "        [0.1729],\n",
      "        [0.7328],\n",
      "        [0.5855],\n",
      "        [0.4271],\n",
      "        [0.9278],\n",
      "        [0.0270],\n",
      "        [0.7859],\n",
      "        [0.1877],\n",
      "        [0.1290],\n",
      "        [0.1066],\n",
      "        [0.5981],\n",
      "        [0.5134],\n",
      "        [0.7837],\n",
      "        [0.4812],\n",
      "        [0.4392],\n",
      "        [0.9949],\n",
      "        [0.7562],\n",
      "        [0.1486],\n",
      "        [0.1757],\n",
      "        [0.8887],\n",
      "        [0.3707],\n",
      "        [0.6003],\n",
      "        [0.0937],\n",
      "        [0.3015],\n",
      "        [0.4735],\n",
      "        [0.8279],\n",
      "        [0.1921],\n",
      "        [0.0696],\n",
      "        [0.8954],\n",
      "        [0.5195],\n",
      "        [0.6857],\n",
      "        [0.8753],\n",
      "        [0.4735],\n",
      "        [0.2253],\n",
      "        [0.4000],\n",
      "        [0.6433],\n",
      "        [0.2653],\n",
      "        [0.2931],\n",
      "        [0.3792],\n",
      "        [0.4734],\n",
      "        [0.7896],\n",
      "        [0.0485],\n",
      "        [0.0163],\n",
      "        [0.6544],\n",
      "        [0.4058],\n",
      "        [0.6917],\n",
      "        [0.4586],\n",
      "        [0.4712],\n",
      "        [0.7832],\n",
      "        [0.8428],\n",
      "        [0.1290],\n",
      "        [0.2566],\n",
      "        [0.6857],\n",
      "        [0.0314],\n",
      "        [0.2040],\n",
      "        [0.7721],\n",
      "        [0.8060],\n",
      "        [0.8177],\n",
      "        [0.9601],\n",
      "        [0.8902],\n",
      "        [0.0631],\n",
      "        [0.9772],\n",
      "        [0.5060],\n",
      "        [0.0616],\n",
      "        [0.5278],\n",
      "        [0.3667],\n",
      "        [0.2457],\n",
      "        [0.2028],\n",
      "        [0.1425],\n",
      "        [0.7716],\n",
      "        [0.5422],\n",
      "        [0.5854],\n",
      "        [0.5008],\n",
      "        [0.9157],\n",
      "        [0.2493],\n",
      "        [0.8124],\n",
      "        [0.6020],\n",
      "        [0.8061],\n",
      "        [0.9096],\n",
      "        [0.5035],\n",
      "        [0.1508],\n",
      "        [0.8711],\n",
      "        [0.6099],\n",
      "        [0.8261],\n",
      "        [0.2329],\n",
      "        [0.8850],\n",
      "        [0.8708],\n",
      "        [0.8445],\n",
      "        [0.0083],\n",
      "        [0.8385],\n",
      "        [0.2631],\n",
      "        [0.3397],\n",
      "        [0.7819],\n",
      "        [0.5477],\n",
      "        [0.0575],\n",
      "        [0.8316],\n",
      "        [0.1705],\n",
      "        [0.2574],\n",
      "        [0.3809],\n",
      "        [0.4678],\n",
      "        [0.9877],\n",
      "        [0.3151],\n",
      "        [0.7563],\n",
      "        [0.0993],\n",
      "        [0.9454],\n",
      "        [0.5773],\n",
      "        [0.1926],\n",
      "        [0.2819],\n",
      "        [0.5814],\n",
      "        [0.6416],\n",
      "        [0.5296],\n",
      "        [0.0864],\n",
      "        [0.9467],\n",
      "        [0.8779],\n",
      "        [0.9248],\n",
      "        [0.6711],\n",
      "        [0.3348],\n",
      "        [0.8181],\n",
      "        [0.5830],\n",
      "        [0.8526],\n",
      "        [0.1238],\n",
      "        [0.2038],\n",
      "        [0.3059],\n",
      "        [0.9893],\n",
      "        [0.7907],\n",
      "        [0.2405],\n",
      "        [0.4595],\n",
      "        [0.1737],\n",
      "        [0.4727],\n",
      "        [0.9720],\n",
      "        [0.7999],\n",
      "        [0.6898],\n",
      "        [0.3071],\n",
      "        [0.7319],\n",
      "        [0.1578],\n",
      "        [0.2813],\n",
      "        [0.6689],\n",
      "        [0.9818],\n",
      "        [0.5846],\n",
      "        [0.2202],\n",
      "        [0.2617],\n",
      "        [0.7992],\n",
      "        [0.9015],\n",
      "        [0.7066],\n",
      "        [0.6947],\n",
      "        [0.9585],\n",
      "        [0.1282],\n",
      "        [0.0837],\n",
      "        [0.8028],\n",
      "        [0.9590],\n",
      "        [0.1970],\n",
      "        [0.4006],\n",
      "        [0.5638],\n",
      "        [0.3003],\n",
      "        [0.6423],\n",
      "        [0.5327],\n",
      "        [0.5598],\n",
      "        [0.9658],\n",
      "        [0.2030],\n",
      "        [0.0636],\n",
      "        [0.6168],\n",
      "        [0.2880],\n",
      "        [0.3982],\n",
      "        [0.7475],\n",
      "        [0.2265],\n",
      "        [0.5754],\n",
      "        [0.8405],\n",
      "        [0.3957],\n",
      "        [0.4010],\n",
      "        [0.3838],\n",
      "        [0.1628],\n",
      "        [0.4143],\n",
      "        [0.6626],\n",
      "        [0.0521],\n",
      "        [0.5150],\n",
      "        [0.6490],\n",
      "        [0.5416],\n",
      "        [0.7453],\n",
      "        [0.8982],\n",
      "        [0.4818],\n",
      "        [0.6552],\n",
      "        [0.1655],\n",
      "        [0.2042],\n",
      "        [0.0457],\n",
      "        [0.3534],\n",
      "        [0.1049],\n",
      "        [0.2279],\n",
      "        [0.1114],\n",
      "        [0.1337],\n",
      "        [0.1758],\n",
      "        [0.7767],\n",
      "        [0.3931],\n",
      "        [0.7742],\n",
      "        [0.1082],\n",
      "        [0.1656],\n",
      "        [0.7893],\n",
      "        [0.5262],\n",
      "        [0.0112],\n",
      "        [0.9379],\n",
      "        [0.2209],\n",
      "        [0.8119],\n",
      "        [0.3199],\n",
      "        [0.9523],\n",
      "        [0.4199],\n",
      "        [0.4925],\n",
      "        [0.1067],\n",
      "        [0.8007],\n",
      "        [0.0436],\n",
      "        [0.6843],\n",
      "        [0.1246],\n",
      "        [0.4086],\n",
      "        [0.0298],\n",
      "        [0.4195],\n",
      "        [0.0438],\n",
      "        [0.9796],\n",
      "        [0.9278],\n",
      "        [0.3452],\n",
      "        [0.3944],\n",
      "        [0.8702],\n",
      "        [0.3849],\n",
      "        [0.0301],\n",
      "        [0.0456],\n",
      "        [0.7546],\n",
      "        [0.0669],\n",
      "        [0.4937],\n",
      "        [0.6887],\n",
      "        [0.0166],\n",
      "        [0.9586],\n",
      "        [0.9632],\n",
      "        [0.3596],\n",
      "        [0.9525],\n",
      "        [0.3475],\n",
      "        [0.0360],\n",
      "        [0.4754],\n",
      "        [0.2807],\n",
      "        [0.5158],\n",
      "        [0.1352],\n",
      "        [0.3719],\n",
      "        [0.0508],\n",
      "        [0.0549],\n",
      "        [0.1333],\n",
      "        [0.7395],\n",
      "        [0.1133],\n",
      "        [0.1997],\n",
      "        [0.9125],\n",
      "        [0.8394],\n",
      "        [0.9305],\n",
      "        [0.8098],\n",
      "        [0.8350],\n",
      "        [0.4454],\n",
      "        [0.7394],\n",
      "        [0.7683],\n",
      "        [0.0118],\n",
      "        [0.2862],\n",
      "        [0.2132],\n",
      "        [0.0141],\n",
      "        [0.2230],\n",
      "        [0.6038],\n",
      "        [0.3419],\n",
      "        [0.5278],\n",
      "        [0.1188],\n",
      "        [0.2865],\n",
      "        [0.7836],\n",
      "        [0.3394],\n",
      "        [0.4614],\n",
      "        [0.7574],\n",
      "        [0.4155],\n",
      "        [0.2080],\n",
      "        [0.0168],\n",
      "        [0.2573],\n",
      "        [0.7043],\n",
      "        [0.9436],\n",
      "        [0.8550],\n",
      "        [0.3412],\n",
      "        [0.4490],\n",
      "        [0.9323],\n",
      "        [0.5601],\n",
      "        [0.6444],\n",
      "        [0.4192],\n",
      "        [0.0525],\n",
      "        [0.7238],\n",
      "        [0.3567],\n",
      "        [0.8891],\n",
      "        [0.9005],\n",
      "        [0.5338],\n",
      "        [0.2050],\n",
      "        [0.3585],\n",
      "        [0.3227],\n",
      "        [0.0948],\n",
      "        [0.0028],\n",
      "        [0.2478],\n",
      "        [0.8455],\n",
      "        [0.2865],\n",
      "        [0.7555],\n",
      "        [0.7507],\n",
      "        [0.8620],\n",
      "        [0.4122],\n",
      "        [0.4529],\n",
      "        [0.9548],\n",
      "        [0.6804],\n",
      "        [0.8999],\n",
      "        [0.5936],\n",
      "        [0.5934],\n",
      "        [0.1279],\n",
      "        [0.9931],\n",
      "        [0.5403],\n",
      "        [0.2669],\n",
      "        [0.8161],\n",
      "        [0.8300],\n",
      "        [0.9067],\n",
      "        [0.7507],\n",
      "        [0.3617],\n",
      "        [0.0469],\n",
      "        [0.3296],\n",
      "        [0.4085],\n",
      "        [0.7210],\n",
      "        [0.2603],\n",
      "        [0.7208],\n",
      "        [0.5935],\n",
      "        [0.8504],\n",
      "        [0.9759],\n",
      "        [0.1323],\n",
      "        [0.0093],\n",
      "        [0.6517],\n",
      "        [0.6477],\n",
      "        [0.8527],\n",
      "        [0.4935],\n",
      "        [0.5547],\n",
      "        [0.0663],\n",
      "        [0.7024],\n",
      "        [0.9284],\n",
      "        [0.9587],\n",
      "        [0.3395],\n",
      "        [0.9032],\n",
      "        [0.7179],\n",
      "        [0.9537],\n",
      "        [0.6775],\n",
      "        [0.2054],\n",
      "        [0.1639],\n",
      "        [0.1635],\n",
      "        [0.5291],\n",
      "        [0.8172],\n",
      "        [0.6023],\n",
      "        [0.2351],\n",
      "        [0.5216],\n",
      "        [0.5028],\n",
      "        [0.9837],\n",
      "        [0.1166],\n",
      "        [0.4665],\n",
      "        [0.0461],\n",
      "        [0.1998],\n",
      "        [0.5289],\n",
      "        [0.8692],\n",
      "        [0.0512],\n",
      "        [0.4586],\n",
      "        [0.1718],\n",
      "        [0.1715],\n",
      "        [0.6502],\n",
      "        [0.3538],\n",
      "        [0.0508],\n",
      "        [0.1488],\n",
      "        [0.5100],\n",
      "        [0.7484],\n",
      "        [0.8247],\n",
      "        [0.8601],\n",
      "        [0.5775],\n",
      "        [0.3299],\n",
      "        [0.3092],\n",
      "        [0.8703],\n",
      "        [0.3273],\n",
      "        [0.3405],\n",
      "        [0.5539],\n",
      "        [0.8158],\n",
      "        [0.3229],\n",
      "        [0.5402],\n",
      "        [0.4922],\n",
      "        [0.3397],\n",
      "        [0.8928],\n",
      "        [0.7362],\n",
      "        [0.2780],\n",
      "        [0.7532],\n",
      "        [0.1357],\n",
      "        [0.2525],\n",
      "        [0.1864],\n",
      "        [0.1217],\n",
      "        [0.4948],\n",
      "        [0.9530],\n",
      "        [0.1865],\n",
      "        [0.1929],\n",
      "        [0.7962],\n",
      "        [0.5711],\n",
      "        [0.9238],\n",
      "        [0.4624],\n",
      "        [0.2869],\n",
      "        [0.6839],\n",
      "        [0.2538],\n",
      "        [0.0956],\n",
      "        [0.5874],\n",
      "        [0.3320],\n",
      "        [0.3013],\n",
      "        [0.1387],\n",
      "        [0.6643],\n",
      "        [0.3553],\n",
      "        [0.8619],\n",
      "        [0.2261],\n",
      "        [0.4325],\n",
      "        [0.1159],\n",
      "        [0.0028],\n",
      "        [0.6696],\n",
      "        [0.6190],\n",
      "        [0.7045],\n",
      "        [0.6525],\n",
      "        [0.0842],\n",
      "        [0.3346],\n",
      "        [0.8977],\n",
      "        [0.6811],\n",
      "        [0.4660],\n",
      "        [0.2959],\n",
      "        [0.1549],\n",
      "        [0.6952],\n",
      "        [0.9191],\n",
      "        [0.2699],\n",
      "        [0.8605],\n",
      "        [0.6775],\n",
      "        [0.5548],\n",
      "        [0.9808],\n",
      "        [0.3916],\n",
      "        [0.1398],\n",
      "        [0.9764],\n",
      "        [0.5685],\n",
      "        [0.5685],\n",
      "        [0.4183],\n",
      "        [0.1368],\n",
      "        [0.7969],\n",
      "        [0.7200],\n",
      "        [0.2751],\n",
      "        [0.9492],\n",
      "        [0.2262],\n",
      "        [0.5314],\n",
      "        [0.8777],\n",
      "        [0.0780],\n",
      "        [0.3874],\n",
      "        [0.9758],\n",
      "        [0.9743],\n",
      "        [0.1884],\n",
      "        [0.4868],\n",
      "        [0.3566],\n",
      "        [0.2923],\n",
      "        [0.5121],\n",
      "        [0.3609],\n",
      "        [0.2238],\n",
      "        [0.5396],\n",
      "        [0.5043],\n",
      "        [0.1491],\n",
      "        [0.3814],\n",
      "        [0.6107],\n",
      "        [0.9974],\n",
      "        [0.2708],\n",
      "        [0.9758],\n",
      "        [0.1833],\n",
      "        [0.1306],\n",
      "        [0.0104],\n",
      "        [0.1395],\n",
      "        [0.6647],\n",
      "        [0.6777],\n",
      "        [0.3246],\n",
      "        [0.1514],\n",
      "        [0.6424],\n",
      "        [0.9477],\n",
      "        [0.4670],\n",
      "        [0.5689],\n",
      "        [0.7957]], grad_fn=<UnsqueezeBackward0>)\n",
      "torch.Size([1000, 181])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (181) must match the size of tensor b (1000) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# train jointly\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      3\u001b[0m     x\u001b[39m=\u001b[39;49mx_train,\n\u001b[0;32m      4\u001b[0m     y\u001b[39m=\u001b[39;49my_train,\n\u001b[0;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      7\u001b[0m     callbacks\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m      8\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m     validation_data\u001b[39m=\u001b[39;49m[x_val, y_val],\n\u001b[0;32m     10\u001b[0m     early_stopping\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m     rel_save_thresh\u001b[39m=\u001b[39;49mrel_save_thresh)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_EQL_BINNS\\Notebooks\\..\\Modules\\Utils\\ModelWrapper.py:249\u001b[0m, in \u001b[0;36mModelWrapper.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, early_stopping, best_train_loss, best_val_loss, include_val_aug, include_val_reg, lr_dec_epoch, lr_dec_prop, rel_save_thresh)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39m# update model parameters\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure)\n\u001b[0;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(closure())\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\torch\\optim\\adam.py:118\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 118\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    121\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_EQL_BINNS\\Notebooks\\..\\Modules\\Utils\\ModelWrapper.py:234\u001b[0m, in \u001b[0;36mModelWrapper.fit.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    231\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x_true)\n\u001b[0;32m    233\u001b[0m \u001b[39m# compute loss and optional regularization\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(y_pred, y_true)\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregularizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_reg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregularizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \n\u001b[0;32m    237\u001b[0m                                             x_true,\n\u001b[0;32m    238\u001b[0m                                             y_true,\n\u001b[0;32m    239\u001b[0m                                             y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_EQL_BINNS\\Notebooks\\..\\Modules\\Models\\BuildBINNs.py:1346\u001b[0m, in \u001b[0;36mMLPComponentsCovasim.loss\u001b[1;34m(self, pred, true)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_weight \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1345\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_d:\n\u001b[1;32m-> 1346\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_loss_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_weight \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpde_loss(inputs_rand, outputs_rand)\n\u001b[0;32m   1347\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_loss_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_weight \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_loss_no_d(inputs_rand, outputs_rand)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_EQL_BINNS\\Notebooks\\..\\Modules\\Models\\BuildBINNs.py:1262\u001b[0m, in \u001b[0;36mMLPComponentsCovasim.pde_loss\u001b[1;34m(self, inputs, outputs, return_mean)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta_a_loss\u001b[39m.\u001b[39mshape)\n\u001b[0;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m return_mean:\n\u001b[1;32m-> 1262\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmean(pde_loss  \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meta_a_loss \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meta_y_loss \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtau_a_loss \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtau_y_loss)\n\u001b[0;32m   1263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1264\u001b[0m     \u001b[39mreturn\u001b[39;00m pde_loss\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (181) must match the size of tensor b (1000) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# train jointly\n",
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=None,\n",
    "    verbose=1,\n",
    "    validation_data=[x_val, y_val],\n",
    "    early_stopping=20000,\n",
    "    rel_save_thresh=rel_save_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

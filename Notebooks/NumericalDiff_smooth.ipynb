{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Differentiation of Denoised Data from Sample Mean of Multiple Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import joblib\n",
    "import datetime\n",
    "sys.path.append('../')\n",
    "\n",
    "from Modules.Utils.ModelWrapper import ModelWrapper\n",
    "from Modules.Models.BuildBINNs import MLPComponentsCV # MLPComponentsCovasim, MLPComponentsCovasim2\n",
    "from Modules.Utils.Imports import *\n",
    "\n",
    "import Modules.Loaders.DataFormatter as DF\n",
    "from utils import plot_loss_convergence, get_case_name\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(GetLowestGPU(pick_from=[0,1,2,3]))\n",
    "\n",
    "def to_torch(ndarray):\n",
    "    arr = torch.tensor(ndarray, dtype=torch.float)\n",
    "    arr.requires_grad_(True)\n",
    "    arr = arr.to(device)\n",
    "    return arr\n",
    "\n",
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate BINN model parameters and path\n",
    "path = '../Data/covasim_data/drums_data/'\n",
    "population = 50000\n",
    "test_prob = 0.1\n",
    "trace_prob = 0.3\n",
    "keep_d = True\n",
    "retrain = False\n",
    "dynamic = True\n",
    "n_runs = 1000\n",
    "chi_type = 'piecewise'\n",
    "case_name = get_case_name(population, test_prob, trace_prob, keep_d, dynamic=dynamic, chi_type=chi_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = DF.load_covasim_data_drums(path, population, keep_d, case_name + '_' + str(n_runs), plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>E</th>\n",
       "      <th>A</th>\n",
       "      <th>Y</th>\n",
       "      <th>D</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49900</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49900</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49899</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49889</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49875</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>20</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>28820</td>\n",
       "      <td>310</td>\n",
       "      <td>232</td>\n",
       "      <td>173</td>\n",
       "      <td>216</td>\n",
       "      <td>186</td>\n",
       "      <td>7</td>\n",
       "      <td>19941</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>28779</td>\n",
       "      <td>302</td>\n",
       "      <td>250</td>\n",
       "      <td>170</td>\n",
       "      <td>212</td>\n",
       "      <td>176</td>\n",
       "      <td>4</td>\n",
       "      <td>19991</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>28733</td>\n",
       "      <td>290</td>\n",
       "      <td>266</td>\n",
       "      <td>172</td>\n",
       "      <td>212</td>\n",
       "      <td>177</td>\n",
       "      <td>2</td>\n",
       "      <td>20032</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>28712</td>\n",
       "      <td>273</td>\n",
       "      <td>261</td>\n",
       "      <td>180</td>\n",
       "      <td>200</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>20084</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>28701</td>\n",
       "      <td>247</td>\n",
       "      <td>256</td>\n",
       "      <td>180</td>\n",
       "      <td>210</td>\n",
       "      <td>154</td>\n",
       "      <td>5</td>\n",
       "      <td>20131</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         S    T    E    A    Y    D  Q      R    F\n",
       "0    49900    0  100    0    0    0  0      0    0\n",
       "1    49900    0  100    0    0    0  0      0    0\n",
       "2    49899    0   93    2    6    0  0      0    0\n",
       "3    49889    0   84   10   17    0  0      0    0\n",
       "4    49875    0   71   20   33    1  0      0    0\n",
       "..     ...  ...  ...  ...  ...  ... ..    ...  ...\n",
       "178  28820  310  232  173  216  186  7  19941  115\n",
       "179  28779  302  250  170  212  176  4  19991  116\n",
       "180  28733  290  266  172  212  177  2  20032  116\n",
       "181  28712  273  261  180  200  172  2  20084  116\n",
       "182  28701  247  256  180  210  154  5  20131  116\n",
       "\n",
       "[183 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(params['data'])\n",
    "data_smooth = np.mean(data, axis=0)\n",
    "data_smooth = (data_smooth / params['population'])\n",
    "\n",
    "N = len(data_smooth) - 2\n",
    "u = to_torch(data_smooth[1:N+1,:])\n",
    "\n",
    "t_max_real = N\n",
    "t = np.arange(N)[:,None] + 1\n",
    "params.pop('data')\n",
    "\n",
    "tracing_array = params['tracing_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_front = data_smooth[:N,:]\n",
    "u_back = data_smooth[2:,:]\n",
    "ut = to_torch((u_back - u_front) / 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/val and convert to torch\n",
    "split = int(0.8*N)\n",
    "# generate shuffled array of indices from 0 to N-1\n",
    "p = np.random.permutation(N)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_tensor = torch.cat([u[:,:,None], ut[:,:,None]], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teddy\\AppData\\Local\\Temp\\ipykernel_7944\\3332756441.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  arr = torch.tensor(ndarray, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "x_train = to_torch((p[:split] + 1)/(N-1))\n",
    "# assign y_train to be values corresponding to x_train of size int(0.8 * N)\n",
    "y_train = to_torch(u[(p[:split]).flatten()])\n",
    "# assign x_val to be randomly shuffled days from 1 to 182 of size int(0.2 * N)\n",
    "x_val = to_torch((p[split:] + 1)/(N-1))\n",
    "# assign y_val to be values corresponding to y_val of size int(0.2 * N)\n",
    "y_val = to_torch(u[(p[split:]).flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    data = np.array(params['data'])\n",
    "    data_smooth = np.mean(data, axis=0)\n",
    "    data_smooth = (data_smooth / params['population'])\n",
    "\n",
    "    N = len(data_smooth)\n",
    "\n",
    "    smooth_data = to_torch(data_smooth[1:N-1,:])\n",
    "\n",
    "    t_max = N - 1\n",
    "    t = np.arange(1, N - 1)[:,None]\n",
    "    # indices = (t - 1)[:,0]\n",
    "    params.pop('data')\n",
    "\n",
    "    # computer numerically approximated derivatives\n",
    "    M_front = data_smooth[:N-2,:]\n",
    "    M_back = data_smooth[2:,:]\n",
    "    derivs = (M_back - M_front) / 2.\n",
    "    ut = to_torch(derivs)\n",
    "\n",
    "    # split into train/val and convert to torch\n",
    "    split = int(0.8*N)\n",
    "    # generate shuffled array of indices from 0 to N-1\n",
    "    p = np.random.permutation(N-2) + 1\n",
    "    # assign x_train to be randomly shuffled days from 1 to 182 of size int(0.8 * N)\n",
    "    x_train = to_torch((p[:split][:,None] + 1)/(N-1))\n",
    "    # assign y_train to be values corresponding to x_train of size int(0.8 * N)\n",
    "    y_train = to_torch(data_smooth[p[:split]])\n",
    "    # assign x_val to be randomly shuffled days from 1 to 182 of size int(0.2 * N)\n",
    "    x_val = to_torch((p[split:][:,None] + 1)/(N-1))\n",
    "    # assign y_val to be values corresponding to y_val of size int(0.2 * N)\n",
    "    y_val = to_torch(data_smooth[p[split:]])\n",
    "\n",
    "    tracing_array = params['tracing_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(data_smooth.shape)\n",
    "    print(smooth_data.shape)\n",
    "    print(t_max)\n",
    "    print(N)\n",
    "    print(split)\n",
    "    print(p.shape)\n",
    "    print(p.min())\n",
    "    print(p.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- The shape of `derivs` is $(181, 9)$ since we used the method of central differences to estimate the derivatives for data of shape $(183, 9)$.\n",
    "- `t` are the time points of each of the `derivs` points.\n",
    "- `indices` is the array of the index points for each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSdt = ut[:,0]\n",
    "dTdt = ut[:,1]\n",
    "dEdt = ut[:,2]\n",
    "dAdt = ut[:,3]\n",
    "dYdt = ut[:,4]\n",
    "dDdt = ut[:,5]\n",
    "dQdt = ut[:,6]\n",
    "dRdt = ut[:,7]\n",
    "dFdt = ut[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teddy\\AppData\\Local\\Temp\\ipykernel_7944\\1775666219.py:3: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "plot = False\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "if plot:\n",
    "    for i in range(9):\n",
    "        if i==0:\n",
    "            plt.title('$dSdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dSdt', color='b')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dSdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dSdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==1:\n",
    "            plt.title('$dTdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dTdt*50000, label='dTdt', color='b')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dTdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dTdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==2:\n",
    "            plt.title('$dEdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dEdt', color='y')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dEdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dSEt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==3:\n",
    "            plt.title('$dAdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dAdt', color='r')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dAdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dAdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==4:\n",
    "            plt.title('$dYdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dYdt', color='r')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dYdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dYdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==5:\n",
    "            plt.title('$dDdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dDdt', color='m')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dDdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dDdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==6:\n",
    "            plt.title('$dQdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dQdt', color='m')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dQdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dQdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==7:\n",
    "            plt.title('$dRdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dRdt', color='g')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dRdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dRdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "        if i==8:\n",
    "            plt.title('$dFdt$ versus time (days)')\n",
    "            plt.plot(t_divs, dSdt*50000, label='dFdt', color='k')\n",
    "            plt.xlabel('Time (days)')\n",
    "            plt.ylabel('$dFdt$')\n",
    "            plt.legend()\n",
    "            plt.savefig('../Notebooks/figs/drums/' + case_name + '_' + str(n_runs) + '_dFdt' + '.png')\n",
    "            # plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate save path\n",
    "mydir = os.path.join('../models/covasim', datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(mydir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPComponentsCV(\n",
       "  (eta_func): infect_rate_MLP(\n",
       "    (mlp): BuildMLP(\n",
       "      (activation): ReLU()\n",
       "      (output_activation): Sigmoid()\n",
       "      (MLP): Sequential(\n",
       "        (0): Linear(in_features=3, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (beta_func): beta_MLP(\n",
       "    (mlp): BuildMLP(\n",
       "      (activation): ReLU()\n",
       "      (output_activation): Sigmoid()\n",
       "      (MLP): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (tau_func): tau_MLP(\n",
       "    (mlp): BuildMLP(\n",
       "      (activation): ReLU()\n",
       "      (output_activation): Sigmoid()\n",
       "      (MLP): Sequential(\n",
       "        (0): Linear(in_features=2, out_features=256, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=1, bias=True)\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize model\n",
    "binn = MLPComponentsCV(params, u_tensor, N, tracing_array, keep_d=keep_d, chi_type=chi_type)\n",
    "binn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = binn.parameters()\n",
    "opt = torch.optim.Adam(parameters, lr=1e-3)\n",
    "os.makedirs(os.path.join(mydir, case_name))\n",
    "model = ModelWrapper(\n",
    "    model=binn,\n",
    "    optimizer=opt,\n",
    "    loss=binn.loss,\n",
    "    augmentation=None,\n",
    "    # scheduler= scheduler,\n",
    "    save_name=os.path.join(mydir, case_name) )\n",
    "model.str_name = 'STEAYDQRF_no_main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the range information before training\n",
    "ranges = [binn.yita_lb, binn.yita_ub, binn.beta_lb, binn.beta_ub, binn.tau_lb, binn.tau_ub]\n",
    "file_name = '_'.join([str(m) for m in ranges])\n",
    "joblib.dump(None, os.path.join(mydir, file_name)) # model.save_folder\n",
    "# if retrain\n",
    "if retrain:\n",
    "    model.load(model.save_name + '_best_val_model', device=device)\n",
    "    model.model.train()\n",
    "    model.save_name += '_retrain'\n",
    "    \n",
    "epochs = int(2000)\n",
    "batch_size = 128\n",
    "rel_save_thresh = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# train jointly\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      3\u001b[0m     x\u001b[39m=\u001b[39;49mx_train,\n\u001b[0;32m      4\u001b[0m     y\u001b[39m=\u001b[39;49my_train,\n\u001b[0;32m      5\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      6\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      7\u001b[0m     callbacks\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m      8\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m     validation_data\u001b[39m=\u001b[39;49m[x_val, y_val],\n\u001b[0;32m     10\u001b[0m     early_stopping\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m,\n\u001b[0;32m     11\u001b[0m     rel_save_thresh\u001b[39m=\u001b[39;49mrel_save_thresh)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_EQL_BINNS\\Notebooks\\..\\Modules\\Utils\\ModelWrapper.py:249\u001b[0m, in \u001b[0;36mModelWrapper.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, early_stopping, best_train_loss, best_val_loss, include_val_aug, include_val_reg, lr_dec_epoch, lr_dec_prop, rel_save_thresh)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39m# update model parameters\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure)\n\u001b[0;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mstep(closure())\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\torch\\optim\\adam.py:118\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 118\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    121\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_EQL_BINNS\\Notebooks\\..\\Modules\\Utils\\ModelWrapper.py:234\u001b[0m, in \u001b[0;36mModelWrapper.fit.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    231\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x_true)\n\u001b[0;32m    233\u001b[0m \u001b[39m# compute loss and optional regularization\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(y_pred, y_true)\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregularizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_reg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregularizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \n\u001b[0;32m    237\u001b[0m                                             x_true,\n\u001b[0;32m    238\u001b[0m                                             y_true,\n\u001b[0;32m    239\u001b[0m                                             y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_EQL_BINNS\\Notebooks\\..\\Modules\\Models\\BuildBINNs.py:1800\u001b[0m, in \u001b[0;36mMLPComponentsCV.loss\u001b[1;34m(self, pred, true)\u001b[0m\n\u001b[0;32m   1797\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs\n\u001b[0;32m   1799\u001b[0m \u001b[39m# randomly sample from input domain\u001b[39;00m\n\u001b[1;32m-> 1800\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrandint(\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_max_real, (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt_max_real, \u001b[39m1\u001b[39;49m), requires_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1801\u001b[0m u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu[t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   1802\u001b[0m ut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mut[t\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "# train jointly\n",
    "model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=None,\n",
    "    verbose=1,\n",
    "    validation_data=[x_val, y_val],\n",
    "    early_stopping=20000,\n",
    "    rel_save_thresh=rel_save_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import joblib\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from Modules.Utils.Imports import *\n",
    "from Modules.Utils.ModelWrapper import ModelWrapper\n",
    "from Modules.Models.BuildBINNs import BINNCovasim\n",
    "\n",
    "import Modules.Loaders.DataFormatter as DF\n",
    "import datetime\n",
    "\n",
    "from utils import plot_loss_convergence, get_case_name\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "device = torch.device(GetLowestGPU(pick_from=[0,1,2,3]))\n",
    "# torch.manual_seed(9099058152467048838)\n",
    "# np.random.seed(1232914967)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Data/covasim_data/'\n",
    "population = 200000\n",
    "test_prob = 0.1\n",
    "trace_prob = 0.3\n",
    "keep_d = True\n",
    "retrain = False\n",
    "dynamic=True\n",
    "chi_type = 'piecewise'\n",
    "case_name = get_case_name(population, test_prob, trace_prob, keep_d, dynamic=dynamic, chi_type=chi_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of replicates\n",
    "n_runs = 3\n",
    "# case_name + '_' + str(n_runs)\n",
    "params = DF.load_covasim_data(path, population, test_prob, trace_prob, keep_d, case_name + '_' + str(n_runs),plot=True)\n",
    "\n",
    "def to_torch(ndarray):\n",
    "    arr = torch.tensor(ndarray, dtype=torch.float)\n",
    "    arr.requires_grad_(True)\n",
    "    arr = arr.to(device)\n",
    "    return arr\n",
    "\n",
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate save path\n",
    "mydir = os.path.join('../models/covasim', datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "os.makedirs(mydir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249 | Train loss = 1.6165e+04 | Val loss = 4.5820e+03 | Elapsed = 0:28:12             \n",
      "Epoch 175 | Train loss = 1.6824e+04 | Val loss = 4.6338e+03 | Remaining = 0:09:18           "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m         model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     74\u001b[0m         \u001b[39m# model.save_name += '_' + str(i)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m     \u001b[39m# train jointly\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     78\u001b[0m         x\u001b[39m=\u001b[39;49mx_train,\n\u001b[0;32m     79\u001b[0m         y\u001b[39m=\u001b[39;49my_train,\n\u001b[0;32m     80\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     81\u001b[0m         epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     82\u001b[0m         callbacks\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     83\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     84\u001b[0m         validation_data\u001b[39m=\u001b[39;49m[x_val, y_val],\n\u001b[0;32m     85\u001b[0m         early_stopping\u001b[39m=\u001b[39;49m\u001b[39m40000\u001b[39;49m,\n\u001b[0;32m     86\u001b[0m         rel_save_thresh\u001b[39m=\u001b[39;49mrel_save_thresh)\n\u001b[0;32m     88\u001b[0m \u001b[39m# # fitting performance on training data\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m# y_train_pred = to_numpy(model.predict(x_train))\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[0;32m     91\u001b[0m     \u001b[39m# load training errors\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     total_train_losses \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain_loss_list\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_BINNs\\COVASIM_BINNs\\Notebooks\\..\\Modules\\Utils\\ModelWrapper.py:373\u001b[0m, in \u001b[0;36mModelWrapper.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, early_stopping, best_train_loss, best_val_loss, include_val_aug, include_val_reg, lr_dec_epoch, lr_dec_prop, rel_save_thresh)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot(x_true, y_true, y_pred, epoch, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    372\u001b[0m \u001b[39m# compute loss \u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(y_pred, y_true)\n\u001b[0;32m    375\u001b[0m \u001b[39m# optionally include regularization in val loss\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m include_val_reg \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregularizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_BINNs\\COVASIM_BINNs\\Notebooks\\..\\Modules\\Models\\BuildBINNs.py:820\u001b[0m, in \u001b[0;36mBINNCovasim.loss\u001b[1;34m(self, pred, true)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_weight \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    819\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_d:\n\u001b[1;32m--> 820\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_loss_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_weight \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpde_loss(inputs_rand, outputs_rand)\n\u001b[0;32m    821\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    822\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_loss_val \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_weight \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpde_loss_no_d(inputs_rand, outputs_rand)\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_BINNs\\COVASIM_BINNs\\Notebooks\\..\\Modules\\Models\\BuildBINNs.py:669\u001b[0m, in \u001b[0;36mBINNCovasim.pde_loss\u001b[1;34m(self, inputs, outputs, return_mean)\u001b[0m\n\u001b[0;32m    667\u001b[0m new_d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmu \u001b[39m*\u001b[39m y \u001b[39m+\u001b[39m tau \u001b[39m*\u001b[39m q\n\u001b[0;32m    668\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_com):\n\u001b[1;32m--> 669\u001b[0m     d1 \u001b[39m=\u001b[39m Gradient(u[:, i], inputs, order\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    670\u001b[0m     ut \u001b[39m=\u001b[39m d1[:, \u001b[39m0\u001b[39m][:, \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m    671\u001b[0m     LHS \u001b[39m=\u001b[39m ut \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_max_real\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\Documents\\UG Research\\DRUMS\\COVASIM_BINNs\\COVASIM_BINNs\\Notebooks\\..\\Modules\\Utils\\Gradient.py:25\u001b[0m, in \u001b[0;36mGradient\u001b[1;34m(outputs, inputs, order)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# compute gradients sequentially until order is reached\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(order):\n\u001b[1;32m---> 25\u001b[0m     grads \u001b[39m=\u001b[39m grad(outputs, inputs, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     26\u001b[0m     outputs \u001b[39m=\u001b[39m grads\u001b[39m.\u001b[39msum()\n\u001b[0;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m grads\n",
      "File \u001b[1;32mc:\\Users\\Teddy\\anaconda3\\envs\\reu_env\\lib\\site-packages\\torch\\autograd\\__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    277\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[0;32m    278\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracing_array = params['tracing_array']\n",
    "epochs =  250 # int(10e4)\n",
    "batch_size = 128\n",
    "rel_save_thresh = 0.05\n",
    "\n",
    "# split into train/val and convert to torch\n",
    "for i in range(len(params['data'])): # loop through each sample\n",
    "    data = params['data'][i]\n",
    "    data = (data / params['population']).to_numpy()\n",
    "    # params.pop('data')\n",
    "    N = len(data) # number of days\n",
    "    split = int(0.8*N)\n",
    "    p = np.random.permutation(N)\n",
    "    x_train = to_torch(p[:split][:, None]/(N-1))\n",
    "    y_train = to_torch(data[p[:split]])\n",
    "    x_val = to_torch(p[split:][:, None]/(N-1))\n",
    "    y_val = to_torch(data[p[split:]])\n",
    "\n",
    "    # initialize model\n",
    "    binn = BINNCovasim(params, N - 1, tracing_array, keep_d=keep_d, chi_type=chi_type)\n",
    "    binn.to(device)\n",
    "\n",
    "    # compile\n",
    "    parameters = binn.parameters()\n",
    "    opt = torch.optim.Adam(parameters, lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=5e3)\n",
    "    os.makedirs(os.path.join(mydir, case_name, str(i)))\n",
    "    model = ModelWrapper(\n",
    "        model=binn,\n",
    "        optimizer=opt,\n",
    "        loss=binn.loss,\n",
    "        augmentation=None,\n",
    "        scheduler= scheduler,\n",
    "        save_name=os.path.join(mydir, case_name, str(i)) )\n",
    "    model.str_name = 'STEAYDQRF'\n",
    "\n",
    "    # save the range information before training\n",
    "    ranges = [binn.yita_lb, binn.yita_ub, binn.beta_lb, binn.beta_ub, binn.tau_lb, binn.tau_ub]\n",
    "    file_name = '_'.join([str(m) for m in ranges])\n",
    "    joblib.dump(None, os.path.join(model.save_folder, file_name))\n",
    "    # load initial model after training on the first sample\n",
    "    if i != 0:\n",
    "        model_path = os.path.join(mydir, case_name, str(0))\n",
    "        model.load(model_path + '_best_val_model', device=device)\n",
    "        model.model.train()\n",
    "        # model.save_name += '_' + str(i)\n",
    "\n",
    "    # train jointly\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=None,\n",
    "        verbose=1,\n",
    "        validation_data=[x_val, y_val],\n",
    "        early_stopping=40000,\n",
    "        rel_save_thresh=rel_save_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting performance on training data\n",
    "y_train_pred = to_numpy(model.predict(x_train))\n",
    "\n",
    "# load training errors\n",
    "total_train_losses = model.train_loss_list\n",
    "total_val_losses = model.val_loss_list\n",
    "\n",
    "plot_loss_convergence(total_train_losses, total_val_losses, rel_save_thresh, model.save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

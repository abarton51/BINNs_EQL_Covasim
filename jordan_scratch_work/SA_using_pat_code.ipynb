{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the purpose of doing sensitivty analysis with our drums data using pat's code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Modules.Utils.DRUMS_Lasso import DRUMS_Lasso\n",
    "from Modules.Utils.Imports import *\n",
    "import Modules.Loaders.DataFormatter as DF\n",
    "\n",
    "from Modules.Utils.Imports import *\n",
    "from Modules.Models.BuildBINNs import BINNCovasim\n",
    "from Modules.Utils.ModelWrapper import ModelWrapper\n",
    "#from Notebooks.utils import utils\n",
    "from jordan_scratch_work.utils import get_case_name\n",
    "#from utils import get_case_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________ following code copied from BINNCovasimEvaluation_dynamic.ipynb ________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(GetLowestGPU(pick_from=[0,1,2,3]))\n",
    "# helper functions\n",
    "def to_torch(x):\n",
    "    return torch.from_numpy(x).float().to(device)\n",
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate BINN model parameters and path\n",
    "path = '../Data/covasim_data/drums_data/'\n",
    "# path = '../Data/covasim_data/xin_data/'\n",
    "\n",
    "population = int(200e3)\n",
    "test_prob = 0.1\n",
    "trace_prob = 0.3\n",
    "keep_d = True\n",
    "retrain = False\n",
    "dynamic = True\n",
    "masking = 0\n",
    "multiple = True\n",
    "parallelb = True\n",
    "n_runs = 1024\n",
    "chi_type = 'piecewise'\n",
    "\n",
    "case_name = get_case_name(population, test_prob, trace_prob, keep_d, dynamic=dynamic, chi_type=chi_type)\n",
    "# yita_lb, yita_ub = 0.2, 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not masking==0:\n",
    "    if masking==1:\n",
    "        case_name = case_name + '_maskingthresh'\n",
    "    elif masking==2:\n",
    "        case_name = case_name + '_maskinguni'\n",
    "    elif masking==3:\n",
    "        case_name = case_name + '_maskingnorm'\n",
    "\n",
    "if multiple:\n",
    "    params = DF.load_covasim_data(path, population, test_prob, trace_prob, keep_d, case_name + '_' + str(n_runs), plot=False)\n",
    "else:\n",
    "    params = DF.load_covasim_data(path, population, test_prob, trace_prob, keep_d, case_name, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/val and convert to torch\n",
    "# multiple==True and parallelb==False means that data is a list and not normalized\n",
    "if multiple and not parallelb:\n",
    "    data = np.mean(params['data'], axis=0)\n",
    "    data = (data / params['population'])\n",
    "# multiple==True and parallelb==True means that the data is a 2d array and normalized\n",
    "elif multiple and parallelb:\n",
    "    data = params['data']\n",
    "# otherwise, the data is from a single simulation and is not normalized\n",
    "else:\n",
    "    data = params['data']\n",
    "    data = (data / params['population']).to_numpy()\n",
    "\n",
    "params.pop('data')\n",
    "\n",
    "N = len(data)\n",
    "t_max = N - 1\n",
    "t = np.arange(N)[:,None]\n",
    "\n",
    "tracing_array = params['tracing_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = '../models/covasim/2023-07-12_11-00-45' # no masking, 200e3 pop, dynamic piecewise, keepd, 1024 avg., 50e3 epochs, lr=1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate BINN model\n",
    "binn = BINNCovasim(params, t_max, tracing_array, keep_d=keep_d).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = binn.parameters()\n",
    "model = ModelWrapper(binn, None, None, save_name=os.path.join(mydir, case_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model weights. if retrain==True then load the retrained model\n",
    "if retrain:\n",
    "    model.save_name += '_retrain'\n",
    "model.save_name += '_best_val'\n",
    "model.load(model.save_name + '_model', device=device)\n",
    "\n",
    "# grab initial condition\n",
    "u0 = data[0, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned surface fitter\n",
    "def surface_fitter(t):\n",
    "    res = binn.surface_fitter(t)\n",
    "    return res\n",
    "\n",
    "# learned contact_rate function\n",
    "def contact_rate(u):\n",
    "    res = binn.eta_func(to_torch(u)) # [:,[0,3,4]]\n",
    "    return to_numpy(res)\n",
    "\n",
    "# learned effective tracing rate function\n",
    "def beta(u):\n",
    "    res = binn.beta_func(to_torch(u))\n",
    "    return to_numpy(res)\n",
    "\n",
    "# learned diagnosis of quarantined rate function\n",
    "def tau(u):\n",
    "    res = binn.tau_func(to_torch(u))\n",
    "    return to_numpy(res)\n",
    "\n",
    "# do regression to figure out contact rate\n",
    "\n",
    "\n",
    "def contact_rate_regression(u):\n",
    "    s, a, y = u[:, 0][:, None], u[:, 1][:, None], u[:, 2][:, None]\n",
    "    features = [np.ones_like(a), s, s**2, a, y] #\n",
    "    features = np.concatenate(features, axis=1)\n",
    "    res = features @ regression_coefs_cr\n",
    "    # res *= 1.4\n",
    "    return res\n",
    "\n",
    "# do regression to figure out tracing rate\n",
    "def beta_regression(u):\n",
    "    a, b = u[:, 0][:, None], u[:, 1][:, None]\n",
    "    features = [np.ones_like(a), a, b] #\n",
    "    features = np.concatenate(features, axis=1)\n",
    "    res = features @ regression_coefs_qt\n",
    "    return res\n",
    "\n",
    "# do regression to figure out diagnoses rate (on quarantined folks)\n",
    "def tau_regression(u):\n",
    "    a, b = u[:, 0][:, None], u[:, 1][:, None]\n",
    "    features = [np.ones_like(a), a, b] #\n",
    "    features = np.concatenate(features, axis=1)\n",
    "    res = features @ regression_coefs_tau\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate PDE. First grab the parameter values from the loaded BINN model.\n",
    "params['yita_lb'] = model.model.yita_lb\n",
    "params['yita_ub'] = model.model.yita_ub\n",
    "params['beta_lb'] = model.model.beta_lb\n",
    "params['beta_ub'] = model.model.beta_ub\n",
    "params['tau_lb'] = model.model.tau_lb\n",
    "params['tau_ub'] = model.model.tau_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (183x9 and 3x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m s, a, y \u001b[39m=\u001b[39m data[:, \u001b[39m0\u001b[39m][:, \u001b[39mNone\u001b[39;00m], data[:, \u001b[39m1\u001b[39m][:, \u001b[39mNone\u001b[39;00m], data[:, \u001b[39m2\u001b[39m][:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m      3\u001b[0m input_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m: s,\n\u001b[1;32m      5\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m : a,\n\u001b[1;32m      6\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mY\u001b[39m\u001b[39m\"\u001b[39m : y\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 9\u001b[0m eta \u001b[39m=\u001b[39m contact_rate(data)\n",
      "Cell \u001b[0;32mIn[83], line 8\u001b[0m, in \u001b[0;36mcontact_rate\u001b[0;34m(u)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcontact_rate\u001b[39m(u):\n\u001b[0;32m----> 8\u001b[0m     res \u001b[39m=\u001b[39m binn\u001b[39m.\u001b[39;49meta_func(to_torch(u)) \u001b[39m# [:,[0,3,4]]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m to_numpy(res)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Drums_reu_2023/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/COVASIM_EQL_BINNS/jordan_scratch_work/../Modules/Models/BuildBINNs.py:76\u001b[0m, in \u001b[0;36meta_NN.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m---> 76\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(inputs)\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Drums_reu_2023/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/COVASIM_EQL_BINNS/jordan_scratch_work/../Modules/Models/BuildMLP.py:80\u001b[0m, in \u001b[0;36mBuildMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     78\u001b[0m     \n\u001b[1;32m     79\u001b[0m     \u001b[39m# run the model\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMLP(x)\n\u001b[1;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Drums_reu_2023/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Drums_reu_2023/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Drums_reu_2023/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Drums_reu_2023/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (183x9 and 3x256)"
     ]
    }
   ],
   "source": [
    " s, a, y = data[:, 0][:, None], data[:, 1][:, None], data[:, 2][:, None]\n",
    "\n",
    "input_dict = {\n",
    "    \"S\": s,\n",
    "    \"A\" : a,\n",
    "    \"Y\" : y\n",
    "}\n",
    "\n",
    "eta = contact_rate(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Drums_reu_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

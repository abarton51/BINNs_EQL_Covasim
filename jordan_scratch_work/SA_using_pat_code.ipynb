{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the purpose of doing sensitivty analysis with our drums data using pat's code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Modules.Utils.DRUMS_Lasso import DRUMS_Lasso\n",
    "from Modules.Utils.Imports import *\n",
    "import Modules.Loaders.DataFormatter as DF\n",
    "\n",
    "from Modules.Utils.Imports import *\n",
    "from Modules.Models.BuildBINNs import BINNCovasim\n",
    "from Modules.Utils.ModelWrapper import ModelWrapper\n",
    "#from Notebooks.utils import utils\n",
    "from jordan_scratch_work.utils import get_case_name\n",
    "#from utils import get_case_name\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________ following code copied from BINNCovasimEvaluation_dynamic.ipynb ________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(GetLowestGPU(pick_from=[0,1,2,3]))\n",
    "# helper functions\n",
    "def to_torch(x):\n",
    "    return torch.from_numpy(x).float().to(device)\n",
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate BINN model parameters and path\n",
    "path = '../Data/covasim_data/drums_data/'\n",
    "# path = '../Data/covasim_data/xin_data/'\n",
    "\n",
    "population = int(200e3)\n",
    "test_prob = 0.1\n",
    "trace_prob = 0.3\n",
    "keep_d = True\n",
    "retrain = False\n",
    "dynamic = True\n",
    "masking = 0\n",
    "multiple = True\n",
    "parallelb = True\n",
    "n_runs = 1024\n",
    "chi_type = 'piecewise'\n",
    "\n",
    "case_name = get_case_name(population, test_prob, trace_prob, keep_d, dynamic=dynamic, chi_type=chi_type)\n",
    "# yita_lb, yita_ub = 0.2, 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not masking==0:\n",
    "    if masking==1:\n",
    "        case_name = case_name + '_maskingthresh'\n",
    "    elif masking==2:\n",
    "        case_name = case_name + '_maskinguni'\n",
    "    elif masking==3:\n",
    "        case_name = case_name + '_maskingnorm'\n",
    "\n",
    "if multiple:\n",
    "    params = DF.load_covasim_data(path, population, test_prob, trace_prob, keep_d, case_name + '_' + str(n_runs), plot=False)\n",
    "else:\n",
    "    params = DF.load_covasim_data(path, population, test_prob, trace_prob, keep_d, case_name, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/val and convert to torch\n",
    "# multiple==True and parallelb==False means that data is a list and not normalized\n",
    "if multiple and not parallelb:\n",
    "    data = np.mean(params['data'], axis=0)\n",
    "    data = (data / params['population'])\n",
    "# multiple==True and parallelb==True means that the data is a 2d array and normalized\n",
    "elif multiple and parallelb:\n",
    "    data = params['data']\n",
    "# otherwise, the data is from a single simulation and is not normalized\n",
    "else:\n",
    "    data = params['data']\n",
    "    data = (data / params['population']).to_numpy()\n",
    "\n",
    "params.pop('data')\n",
    "\n",
    "N = len(data)\n",
    "t_max = N - 1\n",
    "t = np.arange(N)[:,None]\n",
    "\n",
    "tracing_array = params['tracing_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = '../models/covasim/2023-07-12_11-00-45' # no masking, 200e3 pop, dynamic piecewise, keepd, 1024 avg., 50e3 epochs, lr=1e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate BINN model\n",
    "binn = BINNCovasim(params, t_max, tracing_array, keep_d=keep_d).to(device)\n",
    "parameters = binn.parameters()\n",
    "model = ModelWrapper(binn, None, None, save_name=os.path.join(mydir, case_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_min, s_max = data[:,0].min(), data[:,0].max()\n",
    "a_min, a_max = data[:,3].min(), data[:,3].max()\n",
    "y_min, y_max = data[:,4].min(), data[:,4].max()\n",
    "say_min, say_max = (data[:,0] + data[:,3] + data[:,4]).min(), (data[:,0] + data[:,3] + data[:,4]).max()\n",
    "chi_min, chi_max = 0.0, params['eff_ub']\n",
    "# grab value ranges\n",
    "yita_lb, yita_ub = model.model.yita_lb, model.model.yita_ub\n",
    "beta_lb, beta_ub = model.model.beta_lb, model.model.beta_ub\n",
    "tau_lb, tau_ub = model.model.tau_lb, model.model.tau_ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned contact_rate function\n",
    "def contact_rate(u):\n",
    "    res = binn.eta_func(to_torch(u)) # [:,[0,3,4]]\n",
    "    return to_numpy(res)\n",
    "# learned tracing rate function\n",
    "def beta(u):\n",
    "    res = binn.beta_func(to_torch(u))\n",
    "    return to_numpy(res)\n",
    "# learned diagnoses rate of quarantined individuals\n",
    "def tau(u):\n",
    "    res = binn.tau_func(to_torch(u))\n",
    "    return to_numpy(res)\n",
    "\n",
    "\n",
    "## **check me, does it matter that I have references to tau_func and beta_func here? Isn't that something we were trying to avoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contact_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m train_x1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39mmeshgrid(s_grid, a_grid, y_grid))\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m     31\u001b[0m data_x1 \u001b[39m=\u001b[39m get_samples_ct(train_x1)\n\u001b[0;32m---> 33\u001b[0m data_y1 \u001b[39m=\u001b[39m contact_rate(train_x1)\n\u001b[1;32m     34\u001b[0m data_y1 \u001b[39m=\u001b[39m data_y1[:,\u001b[39m0\u001b[39m][:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     36\u001b[0m eta_rhs_values \u001b[39m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m : train_x1[:,\u001b[39m0\u001b[39m],\n\u001b[1;32m     38\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m : train_x1[:,\u001b[39m1\u001b[39m],\n\u001b[1;32m     39\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mY\u001b[39m\u001b[39m'\u001b[39m : train_x1[:,\u001b[39m2\u001b[39m],\n\u001b[1;32m     40\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'contact_rate' is not defined"
     ]
    }
   ],
   "source": [
    "def get_samples_ct(u):\n",
    "    s, a, y =  u[:, 0][:, None], u[:, 1][:, None], u[:, 2][:, None]\n",
    "    candidates = [s, s**2, a, y] # s related terms\n",
    "    # candidates += [a]\n",
    "    # candidates += [y]\n",
    "    # candidates += [chi]\n",
    "    candidates = np.concatenate(candidates, axis=1)\n",
    "    return candidates\n",
    "\n",
    "def get_samples_beta(u):\n",
    "    drf, chi = u[:, 0][:, None], u[:, 1][:, None]\n",
    "    candidates = [drf, chi] # , chi**2\n",
    "    candidates = np.concatenate(candidates, axis=1)\n",
    "    return candidates\n",
    "\n",
    "def get_samples_tau(u):\n",
    "    a, y = u[:, 0][:, None], u[:, 1][:, None]\n",
    "    candidates = [a, y]\n",
    "    candidates = np.concatenate(candidates, axis=1)\n",
    "    return candidates\n",
    "\n",
    "\n",
    "\n",
    "s_grid = np.linspace(s_min, s_max, 10)\n",
    "a_grid = np.linspace(a_min, a_max, 10)\n",
    "y_grid = np.linspace(y_min, y_max, 10)\n",
    "\n",
    "#Eta -------------------------\n",
    "train_x1 = np.array(np.meshgrid(s_grid, a_grid, y_grid)).T.reshape(-1,3)\n",
    "\n",
    "data_x1 = get_samples_ct(train_x1)\n",
    "\n",
    "data_y1 = contact_rate(train_x1)\n",
    "data_y1 = data_y1[:,0][:, None]\n",
    "\n",
    "eta_rhs_values = {\n",
    "    'S' : train_x1[:,0],\n",
    "    'A' : train_x1[:,1],\n",
    "    'Y' : train_x1[:,2],\n",
    "}\n",
    "\n",
    "results_eta = DRUMS_Lasso(input_dict = eta_rhs_values, lhs_values = data_y1) \n",
    "\n",
    "\n",
    "print(\"Equation for Eta: \" + str(results_eta['Equation']))\n",
    "print(\"Eta MSE: \" + str(results_eta[\"MSE\"]))\n",
    "#------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#Beta--------------------------\n",
    "\n",
    "\n",
    "say_grid = np.linspace(0.6, 1.0, 10)\n",
    "chi_grid = np.linspace(chi_min, chi_max, 10)\n",
    "\n",
    "# Create meshgrid from the 1D arrays\n",
    "SAY, XX = np.meshgrid(say_grid, chi_grid)   #SAY is the sum S + A + Y\n",
    "\n",
    "# Reshape and combine the arrays\n",
    "data_x2 = np.column_stack((SAY.ravel(), XX.ravel()))\n",
    "data_beta = beta(data_x2)\n",
    "\n",
    "beta_rhs_values = {\n",
    "    'SAY' : data_x2[:,0],\n",
    "    'X' : data_x2[:,1]\n",
    "}\n",
    "\n",
    "results_beta = DRUMS_Lasso(input_dict = beta_rhs_values, lhs_values = data_beta) \n",
    "\n",
    "\n",
    "print(\"Equation for Beta: \" + str(results_beta['Equation']))\n",
    "print(\"Beta MSE: \" + str(results_beta[\"MSE\"]))\n",
    "\n",
    "#----------------------------------\n",
    "\n",
    "#Tau-------------------------------\n",
    "\n",
    "\n",
    "a_grid = np.linspace(a_min, a_max, 10)\n",
    "y_grid = np.linspace(y_min, y_max, 10)\n",
    "labels = ['A', 'Y']\n",
    "\n",
    "AA, YY = np.meshgrid(a_grid, y_grid)\n",
    "data_x3 = np.column_stack((AA.ravel(), YY.ravel()))\n",
    "data_tau = tau(data_x3)\n",
    "print(\"DATA-TAU\")\n",
    "print(np.shape(data_tau))\n",
    "#******\n",
    "#data_tau = data_tau[:,0].reshape(AA.shape)\n",
    "data_tau = tau_lb + (tau_ub - tau_lb) * data_tau # scaling\n",
    "data_tau = np.round(data_tau, decimals=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(np.shape(data_tau))\n",
    "\n",
    "\n",
    "term_names = ['A', 'Y']\n",
    "\n",
    "\n",
    "\n",
    "tau_rhs_values = {\n",
    "    'A' : data_x3[:,0],\n",
    "    'Y' : data_x3[:,1],\n",
    "}\n",
    "\n",
    "results_tau = DRUMS_Lasso(input_dict = tau_rhs_values, lhs_values = data_tau) \n",
    "\n",
    "\n",
    "print(\"Equation for Tau: \" + str(results_tau['Equation']))\n",
    "print(\"Tau MSE: \" + str(results_tau[\"MSE\"]))\n",
    "print(results_tau[\"Lasso\"].coef_)\n",
    "print(results_tau[\"Lasso\"].intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Drums_reu_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
